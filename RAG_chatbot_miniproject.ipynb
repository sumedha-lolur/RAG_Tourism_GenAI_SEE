{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fc2d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\apps required\\python\\lib\\site-packages (from -r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: langchain in d:\\apps required\\python\\lib\\site-packages (from -r requirements.txt (line 2)) (0.0.321)\n",
      "Requirement already satisfied: faiss-cpu in d:\\apps required\\python\\lib\\site-packages (from -r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: pypdf in d:\\apps required\\python\\lib\\site-packages (from -r requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: tiktoken in d:\\apps required\\python\\lib\\site-packages (from -r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: requests>=2.20 in d:\\apps required\\python\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in d:\\apps required\\python\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in d:\\apps required\\python\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (3.8.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (2.0.21)\n",
      "Requirement already satisfied: anyio<4.0 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (0.0.92)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (2.4.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\apps required\\python\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (8.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\apps required\\python\\lib\\site-packages (from tiktoken->-r requirements.txt (line 5)) (2023.12.25)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\apps required\\python\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\apps required\\python\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\apps required\\python\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\apps required\\python\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\apps required\\python\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\apps required\\python\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\apps required\\python\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\apps required\\python\\lib\\site-packages (from anyio<4.0->langchain->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\apps required\\python\\lib\\site-packages (from anyio<4.0->langchain->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\apps required\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 2)) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\apps required\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\apps required\\python\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\apps required\\python\\lib\\site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 2)) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in d:\\apps required\\python\\lib\\site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\apps required\\python\\lib\\site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apps required\\python\\lib\\site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\apps required\\python\\lib\\site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\apps required\\python\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: colorama in d:\\apps required\\python\\lib\\site-packages (from tqdm->openai->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\apps required\\python\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\apps required\\python\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 2)) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae75ad73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'databutton'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatabutton\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'databutton'"
     ]
    }
   ],
   "source": [
    "import databutton as db\n",
    "import re\n",
    "from io import BytesIO\n",
    "from typing import Tuple, List\n",
    "import pickle\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from pypdf import PdfReader\n",
    "import faiss\n",
    "\n",
    "\n",
    "def parse_pdf(file: BytesIO, filename: str) -> Tuple[List[str], str]:\n",
    "    pdf = PdfReader(file)\n",
    "    output = []\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "        text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "        text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "        output.append(text)\n",
    "    return output, filename\n",
    "\n",
    "\n",
    "def text_to_docs(text: List[str], filename: str) -> List[Document]:\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    page_docs = [Document(page_content=page) for page in text]\n",
    "    for i, doc in enumerate(page_docs):\n",
    "        doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "    doc_chunks = []\n",
    "    for doc in page_docs:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=4000,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "            )\n",
    "            doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "            doc.metadata[\"filename\"] = filename  # Add filename to metadata\n",
    "            doc_chunks.append(doc)\n",
    "    return doc_chunks\n",
    "\n",
    "\n",
    "def docs_to_index(docs, openai_api_key):\n",
    "    index = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_index_for_pdf(pdf_files, pdf_names, openai_api_key):\n",
    "    documents = []\n",
    "    for pdf_file, pdf_name in zip(pdf_files, pdf_names):\n",
    "        text, filename = parse_pdf(BytesIO(pdf_file), pdf_name)\n",
    "        documents = documents + text_to_docs(text, filename)\n",
    "    index = docs_to_index(documents, openai_api_key)\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7522ffb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'databutton'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatabutton\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'databutton'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import databutton as db\n",
    "import streamlit as st\n",
    "import openai\n",
    "from brain import get_index_for_pdf\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load the API key from the .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the title for the Streamlit app\n",
    "st.title(\"RAG enhanced Chatbot\")\n",
    "\n",
    "# # Set up the OpenAI API key from databutton secrets\n",
    "# os.environ[\"OPENAI_API_KEY\"] = db.secrets.get(\"OPENAI_API_KEY\")\n",
    "# openai.api_key = db.secrets.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Cached function to create a vectordb for the provided PDF files\n",
    "@st.cache_data\n",
    "def create_vectordb(files, filenames):\n",
    "    # Show a spinner while creating the vectordb\n",
    "    with st.spinner(\"Vector database\"):\n",
    "        vectordb = get_index_for_pdf(\n",
    "            [file.getvalue() for file in files], filenames, openai.api_key\n",
    "        )\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "# Upload PDF files using Streamlit's file uploader\n",
    "pdf_files = st.file_uploader(\"\", type=\"pdf\", accept_multiple_files=True)\n",
    "\n",
    "# If PDF files are uploaded, create the vectordb and store it in the session state\n",
    "if pdf_files:\n",
    "    pdf_file_names = [file.name for file in pdf_files]\n",
    "    st.session_state[\"vectordb\"] = create_vectordb(pdf_files, pdf_file_names)\n",
    "\n",
    "# Define the template for the chatbot prompt\n",
    "prompt_template = \"\"\"\n",
    "    You are a helpful Assistant who answers to users questions based on multiple contexts given to you.\n",
    "\n",
    "    Keep your answer short and to the point.\n",
    "    \n",
    "    The evidence are the context of the pdf extract with metadata. \n",
    "    \n",
    "    Carefully focus on the metadata specially 'filename' and 'page' whenever answering.\n",
    "    \n",
    "    Make sure to add filename and page number at the end of sentence you are citing to.\n",
    "        \n",
    "    Reply \"Not applicable\" if text is irrelevant.\n",
    "     \n",
    "    The PDF content is:\n",
    "    {pdf_extract}\n",
    "\"\"\"\n",
    "\n",
    "# Get the current prompt from the session state or set a default value\n",
    "prompt = st.session_state.get(\"prompt\", [{\"role\": \"system\", \"content\": \"none\"}])\n",
    "\n",
    "# Display previous chat messages\n",
    "for message in prompt:\n",
    "    if message[\"role\"] != \"system\":\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.write(message[\"content\"])\n",
    "\n",
    "# Get the user's question using Streamlit's chat input\n",
    "question = st.chat_input(\"Ask anything\")\n",
    "\n",
    "# Handle the user's question\n",
    "if question:\n",
    "    vectordb = st.session_state.get(\"vectordb\", None)\n",
    "    if not vectordb:\n",
    "        with st.message(\"assistant\"):\n",
    "            st.write(\"You need to provide a PDF\")\n",
    "            st.stop()\n",
    "\n",
    "    # Search the vectordb for similar content to the user's question\n",
    "    search_results = vectordb.similarity_search(question, k=3)\n",
    "    # search_results\n",
    "    pdf_extract = \"/n \".join([result.page_content for result in search_results])\n",
    "\n",
    "    # Update the prompt with the pdf extract\n",
    "    prompt[0] = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt_template.format(pdf_extract=pdf_extract),\n",
    "    }\n",
    "\n",
    "    # Add the user's question to the prompt and display it\n",
    "    prompt.append({\"role\": \"user\", \"content\": question})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(question)\n",
    "\n",
    "    # Display an empty assistant message while waiting for the response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        botmsg = st.empty()\n",
    "\n",
    "    # Call ChatGPT with streaming and display the response as it comes\n",
    "    response = []\n",
    "    result = \"\"\n",
    "    for chunk in openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=prompt, stream=True\n",
    "    ):\n",
    "        text = chunk.choices[0].get(\"delta\", {}).get(\"content\")\n",
    "        if text is not None:\n",
    "            response.append(text)\n",
    "            result = \"\".join(response).strip()\n",
    "            botmsg.write(result)\n",
    "\n",
    "    # Add the assistant's response to the prompt\n",
    "    prompt.append({\"role\": \"assistant\", \"content\": result})\n",
    "\n",
    "    # Store the updated prompt in the session state\n",
    "    st.session_state[\"prompt\"] = prompt\n",
    "    prompt.append({\"role\": \"assistant\", \"content\": result})\n",
    "\n",
    "    # Store the updated prompt in the session state\n",
    "    st.session_state[\"prompt\"] = prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b47dc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a06ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
